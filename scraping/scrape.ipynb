{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 17:53:52,908 - INFO - Crawling: https://vik.wiki/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://vik.wiki/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 17:53:58,171 - ERROR - Request failed: HTTPSConnectionPool(host='vik.wiki', port=443): Read timed out. (read timeout=5)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class WebCrawler:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.base_domain = urlparse(base_url).netloc\n",
    "        self.visited_urls = set()\n",
    "        self.texts = []\n",
    "\n",
    "    def crawl(self, url):\n",
    "        print(url)\n",
    "        # Normalize the URL by removing the fragment\n",
    "        url = urlparse(url)._replace(fragment=\"\").geturl()\n",
    "\n",
    "        # Ensure the URL has not been visited and is within the same domain\n",
    "        if url in self.visited_urls or urlparse(url).netloc != self.base_domain:\n",
    "            return\n",
    "        logging.info(f\"Crawling: {url}\")\n",
    "        self.visited_urls.add(url)\n",
    "\n",
    "        # Send a GET request to the URL\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"Request failed: {e}\")\n",
    "            return\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract and store the text content of the page\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        self.texts.append({'url': url, 'text': text})\n",
    "\n",
    "        # Find all <a> tags, then recursively crawl each href found\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if not href.startswith('http'):\n",
    "                href = urljoin(url, href)\n",
    "            self.crawl(href)\n",
    "\n",
    "    def get_site_text(self):\n",
    "        # Begin crawling from the base URL\n",
    "        self.crawl(self.base_url)\n",
    "        return self.texts\n",
    "\n",
    "# Example usage\n",
    "crawler = WebCrawler(\"https://vik.wiki/\")\n",
    "site_texts = crawler.get_site_text()\n",
    "# For demonstration purposes, let's just print the URLs collected\n",
    "for page in site_texts:\n",
    "    logging.info(f\"URL: {page['url']}, Text Length: {len(page['text'])}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
